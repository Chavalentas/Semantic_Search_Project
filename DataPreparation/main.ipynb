{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ETLPipelines.scholartomongodb import SemanticScholarToMongoDBPipeline\n",
    "from ETLPipelines.arxivtomongodb import ArxivToMongoDBPipeline\n",
    "from ETLPipelines.mongodbtoabstractchunks import MongoDBPapersToAbstractChunksPipeline\n",
    "from ETLPipelines.mongodbtotitlechunks import MongoDBPapersToTitleChunksPipeline\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from Services.papersservice import PapersService\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from Services.transfomerembeddingservice import TransformerEmbeddingService\n",
    "from Services.basicenglishpreprocessingservice import BasicEnglishPreprocessingService\n",
    "from Services.embeddingservice import EmbeddingService\n",
    "from Services.preprocessingservice import PreprocessingService\n",
    "from Services.helper import Helper\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.cluster import AffinityPropagation, KMeans\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and import nltk packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ./tokenizers...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./tokenizers...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to ./tokenizers...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\", download_dir=\"./tokenizers\")\n",
    "nltk.download(\"stopwords\", download_dir=\"./tokenizers\")\n",
    "nltk.download('punkt_tab', download_dir=\"./tokenizers\")\n",
    "from nltk.corpus import stopwords\n",
    "nltk.data.path.append(\"./tokenizers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL: Semantic Scholar API -> Transform -> Load to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is fetched using a specific API handler that communicates with the API using HTTP protocol (returns a dictionary with specific fields).\n",
    "Afterwards, the dictionary is cleaned and prepared (removing papers with missing abstracts and titles, removing duplicate papers).\n",
    "Following attributes are used:\n",
    "* id\n",
    "* title\n",
    "* abstract\n",
    "* authors\n",
    "* publicationDate\n",
    "\n",
    "The cleaned data is loaded to a MongoDB cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction started: 2024-12-10 19:35:37.449318\n",
      "Extraction ended: 2024-12-10 19:35:44.112930\n",
      "Transformation started: 2024-12-10 19:35:44.112930\n",
      "Transformation ended: 2024-12-10 19:35:44.173905\n",
      "Loading started: 2024-12-10 19:35:44.173905\n",
      "Loading ended: 2024-12-10 19:35:50.469286\n"
     ]
    }
   ],
   "source": [
    "pipeline = SemanticScholarToMongoDBPipeline(\"https://api.semanticscholar.org/graph/v1/paper/search/bulk\", \"long term care\", 3000, 1500)\n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL: Arxiv API -> Transform -> Load to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is fetched using a specific API handler that communicates with the API using HTTP protocol (returns a dictionary with specific fields).\n",
    "Afterwards, the dictionary is cleaned and prepared (removing papers with missing abstracts and titles, removing duplicate papers).\n",
    "Following attributes are used:\n",
    "* id\n",
    "* title\n",
    "* abstract\n",
    "* authors\n",
    "* publicationDate\n",
    "\n",
    "The cleaned data is loaded to a MongoDB cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction started: 2024-12-10 19:35:54.759683\n",
      "Extraction ended: 2024-12-10 19:37:32.197517\n",
      "Transformation started: 2024-12-10 19:37:32.197517\n",
      "Transformation ended: 2024-12-10 19:37:32.492564\n",
      "Loading started: 2024-12-10 19:37:32.492564\n",
      "Loading ended: 2024-12-10 19:37:38.883819\n"
     ]
    }
   ],
   "source": [
    "pipeline = ArxivToMongoDBPipeline(\"http://export.arxiv.org/api/query\", \"long+term+care\", 6000, 100)\n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, different sentence transformers are compared that generate embeddings of different lengths.\n",
    "For this purpose, embeddings with these models will be generated and and clustered using different clustering algorithms.\n",
    "Afterwards, the silhoutte score of these clusters will be calculated.\n",
    "Silhouette score is calculated by averaging the following calculation for every point: (b - a) / max(b, a). </br>\n",
    "b ... average distance to the other clusters</br>\n",
    "a ... average distance to other points within the cluster</br>\n",
    "Silhouette score of 0 = the points are in a cluster with other overlapping clusters or close to other clusters </br>\n",
    "Silhouette score of 1 = the points are in a cluster that is clearly distinguished from other clusters </br>\n",
    "Silhouette score of -1 = the points are in not clearly distinguished clusters, the clustering is bad </br>\n",
    "The algorithm with the best clustering score will be chosen as the best one.\n",
    "The abstract sentences will be used to generate the embeddings.\n",
    "Following algorithms will be compared (detailed explanation in slides): \n",
    "* all-MiniLM-L6-v2\n",
    "* paraphrase-MiniLM-L3-v2\n",
    "* bert-base-nli-mean-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "class ClusteringMetric(ABC):\n",
    "    \"\"\"Represents an abstract clustering metric.\n",
    "\n",
    "    Args:\n",
    "        ABC (_type_): The abstract base class.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def get_labels(self, vectors_list: list) -> np.array:\n",
    "        \"\"\"Calculates the labels using a clustering method\n",
    "        for the given list of vectors.\n",
    "\n",
    "        Args:\n",
    "            vectors_list (list): A list of vectors.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Array of the labels for the given vectors.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class KMeansClusteringMetric(ClusteringMetric):\n",
    "    \"\"\"Represents the K-Means clustering metric.\n",
    "\n",
    "    Args:\n",
    "        ClusteringMetric (_type_): Base abstract class.\n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, random_state: int):\n",
    "        \"\"\"Initializes a new instance of KMeansClusteringMetric.\n",
    "\n",
    "        Args:\n",
    "            k (int): The amount of clusters.\n",
    "            random_state (int): The random state.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Is thrown if the amount of clusters is 0 or less.\n",
    "            ValueError: Is thrown if the random state is negative.\n",
    "        \"\"\"\n",
    "        Helper.ensure_type(k, int, \"k must be of type int!\")\n",
    "        Helper.ensure_type(random_state, int, \"random_state must be an int!\")\n",
    "\n",
    "        if k <= 0:\n",
    "            raise ValueError(\"k cannot be negative or less than 0!\")\n",
    "\n",
    "        if random_state < 0:\n",
    "            raise ValueError(\"random_state cannot be negative!\")\n",
    "        \n",
    "        self.k = k\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def get_labels(self, vectors_list: list) -> np.array:\n",
    "        \"\"\"Calculates the labels for the given vectors \n",
    "        using K-Means.\n",
    "\n",
    "        Args:\n",
    "            vectors_list (list): A list of vectors. \n",
    "\n",
    "        Returns:\n",
    "            np.array: Array of the labels for the given vectors.\n",
    "        \"\"\"\n",
    "        Helper.ensure_type(vectors_list, list, \"vectors_list must be a list!\")\n",
    "\n",
    "        kmeans = KMeans(n_clusters=self.k, random_state=self.random_state)\n",
    "        labels = kmeans.fit_predict(vectors_list)\n",
    "        return labels\n",
    "\n",
    "class AffinityClusteringMetric(ClusteringMetric):\n",
    "    \"\"\"Represents the clustering metric that uses affinity propagation.\n",
    "\n",
    "    Args:\n",
    "        ClusteringMetric (_type_):  Base abstract class.\n",
    "    \"\"\"\n",
    "    def __init__(self, metric: str, affinity: str, random_state: int):\n",
    "        \"\"\"Initializes a new instance of AffinityClusteringMetric.\n",
    "\n",
    "        Args:\n",
    "            metric (str): The metric used to calculate distances between vectors.\n",
    "            affinity (str): The metric used for affinity calculation.\n",
    "            random_state (int): The random state.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Is thrown if the random state is negative.\n",
    "        \"\"\"\n",
    "        Helper.ensure_type(metric, str, \"metric must be of type str!\")\n",
    "        Helper.ensure_type(affinity, str, \"affinity must be of type str!\")\n",
    "        Helper.ensure_type(random_state, int, \"random_state must be an int!\")\n",
    "\n",
    "        if random_state < 0:\n",
    "            raise ValueError(\"random_state cannot be negative!\")\n",
    "        \n",
    "        self.metric = metric\n",
    "        self.affinity = affinity\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def get_labels(self, vectors_list: list) -> np.array:\n",
    "        \"\"\"Calculates the labels for the given vectors \n",
    "        using the affinity propagation.\n",
    "\n",
    "        Args:\n",
    "            vectors_list (list): A list of vectors. \n",
    "\n",
    "        Returns:\n",
    "            np.array: Array of the labels for the given vectors.\n",
    "        \"\"\"\n",
    "        Helper.ensure_type(vectors_list, list, \"vectors_list must be a list!\")\n",
    "\n",
    "        similarity_matrix = 1 - pairwise_distances(vectors_list, metric=self.metric)\n",
    "        affinity_propagation = AffinityPropagation(affinity=self.affinity, random_state=self.random_state)\n",
    "        affinity_propagation.fit(similarity_matrix)\n",
    "        labels = affinity_propagation.labels_\n",
    "        return labels\n",
    "\n",
    "def generate_embeddings(input_list: list[str], embedding_service: EmbeddingService, preprocessing_service: PreprocessingService = None) -> list:\n",
    "    \"\"\"Generates embeddings for strings and \n",
    "    also includes possible preprocessing.\n",
    "\n",
    "    Args:\n",
    "        input_list (list[str]): The input list of strings.\n",
    "        embedding_service (EmbeddingService): The embedding service.\n",
    "        preprocessing_service (PreprocessingService, optional): The preprocessing service. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: List of vector embeddings.\n",
    "    \"\"\"\n",
    "    Helper.ensure_list_of_type(input_list, str, \"input_list must be a list!\", \"input_list must contain only string!\")\n",
    "\n",
    "    if preprocessing_service is not None:\n",
    "        Helper.ensure_instance(preprocessing_service, PreprocessingService, \"preprocessing_service must be an instance of PreprocessingService!\")\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for el in input_list:\n",
    "        to_embed = el\n",
    "\n",
    "        if preprocessing_service is not None:\n",
    "            to_embed = preprocessing_service.preprocess(el)\n",
    "\n",
    "        embedding = embedding_service.create_embedding(to_embed)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def get_silhouette_scores(input_list: list[str], transformer_name_dict: dict, clustering_metric: ClusteringMetric, preprocessing_service: PreprocessingService = None) -> pd.DataFrame:\n",
    "    \"\"\"Calculates silhoutte scores for different sentence\n",
    "    transformers.\n",
    "\n",
    "    Args:\n",
    "        input_list (list[str]): A list of strings.\n",
    "        transformer_name_dict (dict): A dictionary containing transformer names as keys\n",
    "        and its embedding services as values.\n",
    "        clustering_metric (ClusteringMetric): Clustering metric used to cluster.\n",
    "        preprocessing_service (PreprocessingService, optional): Preprocessing service. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with sentence transformer model names as attributes\n",
    "        and silhoutte scores as values.\n",
    "    \"\"\"\n",
    "    Helper.ensure_list_of_type(input_list, str, \"input_list must be a list!\", \"input_list must contain only string!\")\n",
    "    Helper.ensure_type(transformer_name_dict, dict, \"transformer_name_dict must be a dict!\")\n",
    "\n",
    "    if preprocessing_service is not None:\n",
    "        Helper.ensure_instance(preprocessing_service, PreprocessingService, \"preprocessing_service must be an instance of PreprocessingService!\")\n",
    "\n",
    "    result = dict()\n",
    "\n",
    "    for model_name in transformer_name_dict.keys():\n",
    "        result[model_name] = []\n",
    "        embedder = transformer_name_dict[model_name]\n",
    "        embeddings = generate_embeddings(input_list, embedder, preprocessing_service)\n",
    "        labels = clustering_metric.get_labels(embeddings)\n",
    "        result[model_name].append(silhouette_score(embeddings, labels))\n",
    "\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Silhouettes frame for different clustering metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_services = {\n",
    "    \"all-MiniLM-L6-v2\" : TransformerEmbeddingService(SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", similarity_fn_name=SimilarityFunction.COSINE)),\n",
    "    \"paraphrase-MiniLM-L3-v2\": TransformerEmbeddingService(SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L3-v2\", similarity_fn_name=SimilarityFunction.COSINE)),\n",
    "    \"bert-base-nli-mean-tokens\": TransformerEmbeddingService(SentenceTransformer(\"sentence-transformers/bert-base-nli-mean-tokens\", similarity_fn_name=SimilarityFunction.COSINE))\n",
    "}\n",
    "\n",
    "load_dotenv()\n",
    "url = os.getenv(\"MONGODB_URL\")\n",
    "db_service = PapersService(url)\n",
    "papers = db_service.get_papers({})\n",
    "abstract_sentences = Helper.concat_lists([nltk.sent_tokenize(el[\"abstract\"], language=\"english\") for el in papers])\n",
    "sws = list(set(stopwords.words('english')))\n",
    "preprocessing_service = BasicEnglishPreprocessingService(sws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only first 20000 sentences to avoid memory problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_sentences = abstract_sentences[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get scores for affinity clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = AffinityClusteringMetric(\"cosine\", \"precomputed\", 42)\n",
    "scores_frame = get_silhouette_scores(abstract_sentences, models_to_services, metric, preprocessing_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <th>paraphrase-MiniLM-L3-v2</th>\n",
       "      <th>bert-base-nli-mean-tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.045169</td>\n",
       "      <td>0.017071</td>\n",
       "      <td>0.024869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   all-MiniLM-L6-v2  paraphrase-MiniLM-L3-v2  bert-base-nli-mean-tokens\n",
       "0          0.045169                 0.017071                   0.024869"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get scores for K-Means clustering (try to find 10 similar meanings and cluster them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(abstract_sentences) // 10\n",
    "metric_2 = KMeansClusteringMetric(k, 42)\n",
    "scores_frame_2 = get_silhouette_scores(abstract_sentences, models_to_services,  metric_2, preprocessing_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <th>paraphrase-MiniLM-L3-v2</th>\n",
       "      <th>bert-base-nli-mean-tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.048464</td>\n",
       "      <td>0.034965</td>\n",
       "      <td>0.036298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   all-MiniLM-L6-v2  paraphrase-MiniLM-L3-v2  bert-base-nli-mean-tokens\n",
       "0          0.048464                 0.034965                   0.036298"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_frame_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** As we can see, both clustering methods, affinity propagation and K-Means have similar Silhouette scores.\n",
    "They are all near 0. This means that points can be in overlapping clusters or equally close to multiple clusters.\n",
    "In this case, the Silhouette score does not give a good estimator, if the clustering is really suitable or not.\n",
    "But in both cases (affinity propagation and K-Means), we see that the sentence transformer all-MiniLM-L6-v2\n",
    "performs the best. Therefore, all-MiniLM-L6-v2 will be chosen as the model for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL: MongoDB -> Transform to abstract chunks with embeddings -> Load to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data abstracts are split into chunks (just sentences, in this case).\n",
    "The sentences are lowercased and stopwords are removed from them.\n",
    "Afterwards, embeddings are generated.\n",
    "Embeddings are stored to MongoDB again (collection abstractChunks).\n",
    "abstractChunks has following attributes:\n",
    "* paperId\n",
    "* chunk\n",
    "* chunkVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction started: 2024-12-10 20:18:09.586992\n",
      "Extraction ended: 2024-12-10 20:18:11.931790\n",
      "Transformation started: 2024-12-10 20:18:11.931790\n",
      "Transformation ended: 2024-12-10 20:25:51.090132\n",
      "Loading started: 2024-12-10 20:25:51.090132\n",
      "Loading ended: 2024-12-10 20:29:12.903062\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", similarity_fn_name=SimilarityFunction.COSINE)\n",
    "embedding_service = TransformerEmbeddingService(embedder)\n",
    "sws = list(set(stopwords.words('english')))\n",
    "preprocessing_service = BasicEnglishPreprocessingService(sws)\n",
    "pipeline = MongoDBPapersToAbstractChunksPipeline(embedding_service, preprocessing_service, 10000, \"../tokenizers\")\n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL: MongoDB -> Transform to title chunks with embeddings -> Load to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data titles are split into chunks (just sentences, in this case).\n",
    "The sentences are lowercased and stopwords are removed from them.\n",
    "Afterwards, embeddings are generated.\n",
    "Embeddings are stored to MongoDB again (collection titleChunks).\n",
    "titleChunks has following attributes:\n",
    "* paperId\n",
    "* chunk\n",
    "* chunkVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction started: 2024-12-10 20:29:14.679990\n",
      "Extraction ended: 2024-12-10 20:29:16.637376\n",
      "Transformation started: 2024-12-10 20:29:16.637376\n",
      "Transformation ended: 2024-12-10 20:30:14.958559\n",
      "Loading started: 2024-12-10 20:30:14.958559\n",
      "Loading ended: 2024-12-10 20:30:41.446858\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", similarity_fn_name=SimilarityFunction.COSINE)\n",
    "embedding_service = TransformerEmbeddingService(embedder)\n",
    "sws = list(set(stopwords.words('english')))\n",
    "preprocessing_service = BasicEnglishPreprocessingService(sws)\n",
    "pipeline = MongoDBPapersToTitleChunksPipeline(embedding_service, preprocessing_service, 10000, \"../tokenizers\")\n",
    "pipeline.execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_search_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
